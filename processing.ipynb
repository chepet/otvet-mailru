{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import nltk\n",
    "from nltk import corpus\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31279\n"
     ]
    }
   ],
   "source": [
    "folder = 'questions'\n",
    "files = [file for file in os.listdir(folder) if file.endswith('.txt')]\n",
    "\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "\n",
    "for file in files:\n",
    "    with open(os.path.join(folder, file)) as f:\n",
    "        questions.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save raw, unprocessed questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('raw_questions.txt', 'w') as f:\n",
    "    f.write('\\n'.join(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_questions = questions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "for q in raw_questions:\n",
    "#     print(q)\n",
    "    questions.append(''.join([ch for ch in q if ch not in string.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Кто знает сайты, где можно качать бесплатные mp3?'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)\n",
    "punctuation = '\"#$%&\\'()*+/:,-.?!;<=>@[\\]^_`{|}~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кто знает сайты где можно качать бесплатные mp3\n",
      "почему так часто встречаються люди хамы\n",
      "почему ногти на больших пальцах рук растут медленнее чем на остальных указательном среднем безымянном и мизинце\n",
      "у кого есть слайдгитара\n",
      "как удалить свой вопрос\n",
      "подскажите пожалйста на сколько опасна киста  в яичнике\n",
      "ктонибудь прыгал с парашютомкакие ощущения\n",
      "можно ли приворожить любимогоесли он тебя любитно боится это сказатьпотомушто его засмеют друзья\n",
      "подскажите какиенибудь прикольные места в петере где можно погулять\n",
      "скажите пожалуйста  кто из звёзд эстрада русской и зарубежной являются вегеторианцами \n"
     ]
    }
   ],
   "source": [
    "questions = [''.join([ch for ch in q if ch not in punctuation]) for q in raw_questions]\n",
    "\n",
    "questions = [q.lower() for q in questions]\n",
    "\n",
    "print('\\n'.join(questions[:5]))\n",
    "print('\\n'.join(questions[-5:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "разве им сам ним ничего и этом всегда можно много через свою мы какой всех на этот без ней надо три когда ты себе от\n"
     ]
    }
   ],
   "source": [
    "stopwords = list(set(corpus.stopwords.words('russian')))\n",
    "\n",
    "print(len(stop_words))\n",
    "print(' '.join(stop_words[:25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def question_to_words(raw_question, stopwords = stopwords):\n",
    "    letters_only = re.sub(\"[^a-zA-Zа-яА-я0-9]\", \" \", raw_question) \n",
    "    words = letters_only.lower().split()                                                  \n",
    "    meaningful_words = [w for w in words if not w in stopwords]   \n",
    "\n",
    "    return(\" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions = [question_to_words(q) for q in raw_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.data.load('tokenizers/punkt/russian.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем `TweetTokenizer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_questions(raw_question, tokenizer = TweetTokenizer(), stopwords = None):\n",
    "    words = tokenizer.tokenize(raw_question.lower())\n",
    "                     \n",
    "    if stopwords:\n",
    "        words = [w for w in words if not w in stopwords]   \n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [tokenize_questions(q) for q in raw_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentenses:  31279\n",
      "words:  354106\n",
      "unique words:  53893\n"
     ]
    }
   ],
   "source": [
    "print('sentenses: ', len(tokens))\n",
    "print('words: ', len([token for sent in tokens for token in sent]))\n",
    "print('unique words: ', len(set([token for sent in tokens for token in sent])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['кто', 'знает', 'сайты', ',', 'где', 'можно', 'качать', 'бесплатные', 'mp3', '?'] \n",
      "\n",
      "['почему', 'так', 'часто', 'встречаються', 'люди', 'хамы', '?'] \n",
      "\n",
      "['почему', 'ногти', 'на', 'больших', 'пальцах', 'рук', 'растут', 'медленнее', ',', 'чем', 'на', 'остальных', '(', 'указательном', ',', 'среднем', ',', 'безымянном', 'и', 'мизинце', ')', '?'] \n",
      "\n",
      "['у', 'кого', 'есть', 'слайд-гитара', '?'] \n",
      "\n",
      "['как', 'удалить', 'свой', 'вопрос', '?'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in tokens[:5]: print(str(t), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Окей, выглядит неплохо, но там наверняка много мусора.\n",
    "\n",
    "* Добавим `reduce_len = True` (вместо 53893 уникальных слов мы получим 53881): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_reduce_len = [tokenize_questions(q, TweetTokenizer(reduce_len = True)) \n",
    "                     for q in raw_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentenses:  31279\n",
      "words:  354106\n",
      "unique words:  53881\n"
     ]
    }
   ],
   "source": [
    "print('sentenses: ', len(tokens_reduce_len))\n",
    "print('words: ', len([token for sent in tokens_reduce_len for token in sent]))\n",
    "print('unique words: ', len(set([token for sent in tokens_reduce_len for token in sent])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'zzzz', '3200000', '0xc0000005', '2024561111', '300000', '500000', '40000р', 'способствуетtttttt', '330000евро', 'pomogiiiiiiiiiiiiite', '6666', '200000', '100000', '40000', '10000тыс', '30000тыс', '1000000', '10000000', '70000', 'soooooooos', 'aaaaaa', '10000', '20000-25000', '20000', 'xxxxx', '150000', '10000к', '0,000011', '150000тдол', '0x0000001a'}\n"
     ]
    }
   ],
   "source": [
    "print(set([token for sent in tokens for token in sent])\n",
    "      .difference(set([token for sent in tokens_reduce_len for token in sent])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Добавим `strip_handles = True` к `reduce_len = True` (вместо 53881 уникальных слов мы получим 53878). \n",
    "\n",
    "(Потеряются слова `{'@zakladkis', '@kiska', '@instagram'}`. На мой взгляд, особой важности в них не было.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_reduce_strip = [tokenize_questions(q, TweetTokenizer(reduce_len = True, strip_handles = True)) \n",
    "                    for q in raw_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentenses:  31279\n",
      "words:  354102\n",
      "unique words:  53878\n"
     ]
    }
   ],
   "source": [
    "print('sentenses: ', len(tokens_reduce_strip))\n",
    "print('words: ', len([token for sent in tokens_reduce_strip for token in sent]))\n",
    "print('unique words: ', len(set([token for sent in tokens_reduce_strip for token in sent])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'@zakladkis', '@kiska', '@instagram'}\n"
     ]
    }
   ],
   "source": [
    "print(set([token for sent in tokens_reduce_len for token in sent])\n",
    "      .difference(set([token for sent in tokens_reduce_strip for token in sent])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут марковчейн для развлечения, пока вопросы парсятся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import markovify\n",
    "# import pymarkovchain\n",
    "\n",
    "# mv = markovify.Text('\\n'.join(questions), state_size = 2)\n",
    "# for _ in range(50): print(mv.make_sentence(), '\\n')\n",
    "\n",
    "# mc = pymarkovchain.MarkovChain()\n",
    "# mc.generateDatabase('\\n'.join(questions))\n",
    "# for _ in range(50): print(mc.generateString() + '?', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
