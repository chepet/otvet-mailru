{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import nltk\n",
    "from nltk import corpus\n",
    "from nltk.tokenize import TweetTokenizer, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127502\n"
     ]
    }
   ],
   "source": [
    "folder = 'questions'\n",
    "files = [file for file in os.listdir(folder) if file.endswith('.txt')]\n",
    "\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_questions = []\n",
    "\n",
    "for file in files:\n",
    "    with open(os.path.join(folder, file)) as f:\n",
    "        raw_questions.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save raw, unprocessed questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('raw_questions.txt', 'w') as f:\n",
    "    f.write('\\n'.join(raw_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Можно ли есть такой творог?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уберём пунктуацию и стоп-слова\n",
    "\n",
    "1\\. Пунктуация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)\n",
    "punctuation = '\"#$%&\\'()*+/:,-.?!;<=>@[\\]^_`{|}~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# questions = [''.join([ch for ch in q if ch not in punctuation]).lower()  for q in raw_questions]\n",
    "# for q in questions[:5]: print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Уберём из предложений также стоп-слова из `nltk.corpus.stopwords.words('russian')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "['ничего', 'какая', 'эти', 'ему', 'не', 'только', 'раз', 'и', 'тебя', 'всех', 'почти', 'этой', 'разве', 'ну', 'куда']\n"
     ]
    }
   ],
   "source": [
    "stopwords = list(set(corpus.stopwords.words('russian')))\n",
    "\n",
    "print(len(stopwords))\n",
    "print(str(stopwords[:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raw_question_to_clean(raw_question, stopwords = stopwords):\n",
    "    \"\"\"\n",
    "    raw_question: str\n",
    "    stopwords: list of str\n",
    "    \n",
    "    returns: str (question without stopwords and punctuation)\n",
    "    e.g:\n",
    "        raw_question = 'С чем будет связана моя будущая работа? кем лучше стать? 25.09.1999'\n",
    "        returns 'связана будущая работа кем стать 25 09 1999'\n",
    "    \"\"\"\n",
    "    \n",
    "    letters_only = re.sub('[^a-zA-Zа-яА-я0-9]', ' ', raw_question) \n",
    "    \n",
    "    words = letters_only.lower().split()                                                  \n",
    "    meaningful_words = [w for w in words if not w in stopwords]   \n",
    "\n",
    "    return(' '.join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = [raw_question_to_clean(q) for q in raw_questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним почищенные (`questions`) и исходные, необработанные вопросы (`raw_questions`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Можно ли есть такой творог?\n",
      "after:  творог\n",
      "\n",
      "before: Возможная стоимость данных предметов\n",
      "after:  возможная стоимость данных предметов\n",
      "\n",
      "before: Можно ли есть такой творог? Кислый на вкус\n",
      "after:  творог кислый вкус\n",
      "\n",
      "before: Драка во сне\n",
      "after:  драка сне\n",
      "\n",
      "before: Что же это за военные в РФ, которых можно спокойно спеленать и вывести за пределы страны?\n",
      "after:  это военные рф которых спокойно спеленать вывести пределы страны\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q, rq in zip(questions[:5], raw_questions[:5]): print('before: {}\\nafter:  {}\\n'.format(rq, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что просто убирать пунктуацию не очень эффективно. Да и без стоп-слов вопросы с Ответов.Mailru кажутся полным бредом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.data.load('tokenizers/punkt/russian.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем `TweetTokenizer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_question(raw_question, tokenizer = TweetTokenizer(), stopwords = None):\n",
    "    \"\"\"\n",
    "    raw_question: str\n",
    "    tokenizer: tokenizer, default: TweetTokenizer()\n",
    "    stopwords: list, default: None\n",
    "    \n",
    "    returns: list of str (tokenized question)\n",
    "    e.g.:\n",
    "        raw_question = Иисус это Вселенная?\n",
    "        returns ['иисус', 'это', 'вселенная', '?']     \n",
    "    \"\"\"\n",
    "    words = tokenizer.tokenize(raw_question.lower())\n",
    "                     \n",
    "    if stopwords:\n",
    "        words = [w for w in words if not w in stopwords]   \n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = [tokenize_question(q) for q in raw_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentenses:  127502\n",
      "words:  1522827\n",
      "unique words:  133062\n"
     ]
    }
   ],
   "source": [
    "print('sentenses: ', len(tokens))\n",
    "print('words: ', len([token for sent in tokens for token in sent]))\n",
    "print('unique words: ', len(set([token for sent in tokens for token in sent])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['можно', 'ли', 'есть', 'такой', 'творог', '?']\n",
      "['возможная', 'стоимость', 'данных', 'предметов']\n",
      "['можно', 'ли', 'есть', 'такой', 'творог', '?', 'кислый', 'на', 'вкус']\n",
      "['драка', 'во', 'сне']\n",
      "['что', 'же', 'это', 'за', 'военные', 'в', 'рф', ',', 'которых', 'можно', 'спокойно', 'спеленать', 'и', 'вывести', 'за', 'пределы', 'страны', '?']\n"
     ]
    }
   ],
   "source": [
    "for t in tokens[:5]: print(str(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Окей, выглядит неплохо, но там наверняка много мусора.\n",
    "\n",
    "* Добавим `reduce_len = True` (вместо 53893 уникальных слов мы получим 53881): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_reduce_len = [tokenize_question(q, TweetTokenizer(reduce_len = True)) \n",
    "                     for q in raw_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentenses:  127502\n",
      "words:  1522820\n",
      "unique words:  133027\n"
     ]
    }
   ],
   "source": [
    "print('sentenses: ', len(tokens_reduce_len))\n",
    "print('words: ', len([token for sent in tokens_reduce_len for token in sent]))\n",
    "print('unique words: ', len(set([token for sent in tokens_reduce_len for token in sent])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,0000\n",
      "30000-45000\n",
      "640000р\n",
      "30000\n",
      "10000\n",
      "5256654\n",
      "2024561111\n",
      "0xc0000023\n",
      "3200000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "for t in list(set([token for sent in tokens for token in sent])\n",
    "              .difference(set([token for sent in tokens_reduce_len for token in sent])))[:10]: print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Добавим `strip_handles = True` к `reduce_len = True` (вместо 53881 уникальных слов мы получим 53878). \n",
    "\n",
    "(Потеряются слова `{'@zakladkis', '@kiska', '@instagram'}`. На мой взгляд, особой важности в них не было.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_reduce_strip = [tokenize_question(q, TweetTokenizer(reduce_len    = True, \n",
    "                                                           strip_handles = True)) \n",
    "                       for q in raw_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentenses:  127502\n",
      "words:  1522810\n",
      "unique words:  133020\n"
     ]
    }
   ],
   "source": [
    "print('sentenses: ', len(tokens_reduce_strip))\n",
    "print('words: ', len([token for sent in tokens_reduce_strip for token in sent]))\n",
    "print('unique words: ', len(set([token for sent in tokens_reduce_strip for token in sent])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@yahoo\n",
      "@yandex\n",
      "@zakladkis\n",
      "@instagram\n",
      "@medi\n",
      "@skypoint\n",
      "@echo\n"
     ]
    }
   ],
   "source": [
    "for t in (set([token for sent in tokens_reduce_len for token in sent])\n",
    "          .difference(set([token for sent in tokens_reduce_strip for token in sent]))): print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_word(token):\n",
    "    return not re.search(r'^[\\w\\d\\-]+$', token) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_okay(token):\n",
    "    token_is_word  = is_word(token)\n",
    "    token_is_punct = token in [',', '-', ':']\n",
    "    \n",
    "    return token_is_word or token_is_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# token_list = tokens_reduce_strip[67]\n",
    "# print(str(token_list))\n",
    "\n",
    "# for token in token_list: print(token, '\\t\\t\\t', re.search(r'^[\\w\\d\\-]+$' ,token) is None)\n",
    "# for token in token_list: print(token, '\\t\\t\\t', is_word(token))\n",
    "# for token in token_list: print(token, '\\t\\t', is_okay(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions_tokenized = []\n",
    "\n",
    "for token_list in tokens_reduce_strip:\n",
    "    token_list_ok = [t for t in token_list if is_okay(t)]\n",
    "    question = ' '.join(token_list_ok)\n",
    "    questions_tokenized.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "можно ли есть такой творог\n",
      "возможная стоимость данных предметов\n",
      "можно ли есть такой творог кислый на вкус\n",
      "драка во сне\n",
      "что же это за военные в рф , которых можно спокойно спеленать и вывести за пределы страны\n"
     ]
    }
   ],
   "source": [
    "for q in questions_tokenized[:5]: print(str(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "А теперь попробуем повторить последовательность действий `tokenize (reduce, strip)`, `is_okay`, `join`, но не с цельными вопросами, как в прошлом случае, а с отдельными предложениями (т.е. если вопрос — «зачем? почему?», будем делать указанные выше действия для «зачем?» и для «почему?» по отдельности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_questions = '\\n'.join(raw_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions_sent_tokenized = sent_tokenize(joined_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Можно ли есть такой творог?',\n",
       " 'Возможная стоимость данных предметов\\nМожно ли есть такой творог?',\n",
       " 'Кислый на вкус\\nДрака во сне\\nЧто же это за военные в РФ, которых можно спокойно спеленать и вывести за пределы страны?',\n",
       " 'кто знает сайты для девочек подростков где можно обсуждать все\\nПатриарх Кирилл объявил Христа и апостолов «неудачниками»!',\n",
       " 'если бы и правда было - нажал на кномпачку ...и секс получил.',\n",
       " 'ты бы скока жал?)))',\n",
       " 'В Киеве отметили 3-летие майдана факельными шествиями, зигами, нацилозунгами, драками поджогами зданий.',\n",
       " 'ВОТ ГДЕ РОМАНТИКА!?',\n",
       " '㋡Чем торгуют на этом мультяшном рынке?',\n",
       " '㋡ςμ⇨⇨⇨\\nкак не сойти с ума в росси и ?']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_sent_tokenized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_reduce_strip = [tokenize_question(q, TweetTokenizer(reduce_len    = True, \n",
    "                                                           strip_handles = True)) \n",
    "                       for q in questions_sent_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительная обработка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_space_before(string):\n",
    "    \"\"\"\n",
    "    string: str\n",
    "    returns: str (without spaces before commas and colons)\n",
    "    e.g.:\n",
    "        string = 'дять , а 2 провода которые вышли от транса куда уйдет'\n",
    "        returns 'дять, а 2 провода которые вышли от транса куда уйдет'\n",
    "    \"\"\"\n",
    "    s = re.sub(' ,', ',', string)\n",
    "    s = re.sub(' :', ':', s)\n",
    "    return  s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут марковчейн для развлечения, пока вопросы парсятся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import markovify\n",
    "import pymarkovchain\n",
    "\n",
    "# mv = markovify.Text('\\n'.join(questions), state_size = 2)\n",
    "# for _ in range(50): print(mv.make_sentence(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mc = pymarkovchain.MarkovChain()\n",
    "mc.generateDatabase('\\n'.join(questions_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- как создать готический сайт?\n",
      "- а она что на четвереньках, очень нужно?\n",
      "- васкогда нить фоткали когда вы поняли, что нужно готовить сына путина к федеральному собранию по отношению ко мне мужчина, какие еще знаки о рождении детей вы хотели изменить в системе увеличить?\n",
      "- срочно?\n",
      "- как накрутить часы в skyrim?\n",
      "- как выглядит грустный человек т е как его написать?\n",
      "- въ способнъ прощать и давать еще один ребус от меня надо?\n",
      "- почему так пишет?\n",
      "- америкосы столько врут, что дяди нет?\n",
      "- безбожное поколение потомков недобитых большевиков и коммунистов вновь готово громить православные храмы?\n",
      "- за это отдать?\n",
      "- что делать?\n",
      "- научите?\n",
      "- наш зато с колен без путина?\n",
      "- а как ты подпитываешь себя в зеркале другое где правда подскажите?\n",
      "- помогите пожалуйста я не из гдз?\n",
      "- транзисторы разных частот?\n",
      "- witcher enhanced edition ошибка при входе на карту придут ли они там размещаться?\n",
      "- а кто тогда такие орбы-плазмоиды, если будет лобовое столкновение?\n",
      "- поверхность земли 22 гр воздушный шар что вы надеетесь?\n",
      "- подчеркните вещества, составьте их список, невозможно достучатся?\n",
      "- поностальгируем немножко для тех кто тянет назад?\n",
      "- во сне того кто мечтает и стремится или у него появится мужчина которым будут отношения с девушкой парень максим 1992?\n",
      "- какой цвет выбрать?\n",
      "- как можно соревноваться с обдолбанными скотами?\n",
      "- какую экипировку надо покупать?\n",
      "- можно даты появления человеческой расы, библии и корана?\n",
      "- смерть диктатора, только на огне, который состоит в гильдии?\n",
      "- детализация звонков в личном на 2017 г?\n",
      "- составьте предложения к схемам пожалуйста?\n",
      "- почему из всех?\n",
      "- ну впрочем как всеглда перед тем как сьесть?\n",
      "- куда звонить?\n",
      "- какой валюте откладывать сбережения по 15-20 т р спасибо?\n",
      "- у пациента в первую очередь?\n",
      "- пойдет такая стрижка?\n",
      "- почему скорость интернета в hmailserver?\n",
      "- нестабильный пинг в кс го в пати, развалим кабины в мм?\n",
      "- какие то магические способности есть?\n",
      "- поедем ли в симс 3 ничего не удалится?\n",
      "- могут быть отношения?\n",
      "- в 14 раз то власть перестала сюсюкаться с нато долго же я сбил?\n",
      "- у вас имеется дома на всякий случай?\n",
      "- почему забыл академика кадырова?\n",
      "- почему государство не хочет в подарок?\n",
      "- мой семейный вопрос на засыпку реально ли там школьникам?\n",
      "- влюбилась знаменитость, которая считает, что делать если мужчина не делает попыток ее в постель?\n",
      "- а, у учителей?\n",
      "- какие затраты?\n",
      "- сочинение по плану описать реку амур?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(50): print('- ' + (remove_space_before(mc.generateString())) + '?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
